import logging
from typing import Dict, Tuple

from core.config import config # New config import
from .llm.base import LLMProviderInterface # Import the interface
from .llm.mistral_provider import MistralLLMProvider # Import concrete Mistral provider

# Remove old Mistral-specific imports if they are now handled by MistralLLMProvider
# import json
# from mistralai import Mistral 
# from mistralai import models as mistral_models # Check if still needed or handled by MistralClient
# from config.mistral_config import MISTRAL_API_KEY, has_valid_api_key # Old config

# Helper to create providers (can be expanded for more providers)
def get_llm_provider(provider_name: str) -> LLMProviderInterface:
    if provider_name.lower() == "mistral":
        # Configuration for Mistral (like API key, model) is handled within MistralLLMProvider using core.config
        return MistralLLMProvider()
    # Add other providers here:
    # elif provider_name.lower() == "openai":
    # return OpenAILLMProvider() 
    else:
        raise ValueError(f"Unsupported LLM provider: {provider_name}")

class QueryGenerator:
    """
    Handles natural language processing and conversion to SQL queries
    by delegating to a configured LLM provider.
    """

    def __init__(self):
        """
        Initialize the QueryGenerator.
        It will load the LLM provider based on application configuration.
        """
        self.logger = logging.getLogger(__name__)
        self.schema_name = config.db_schema_name # Get schema name from config
        
        try:
            self.logger.info(f"Initializing LLM provider: {config.llm_provider_name} for schema: {self.schema_name}")
            self.llm_handler: LLMProviderInterface = get_llm_provider(config.llm_provider_name)
        except ValueError as e:
            self.logger.error(f"Failed to initialize LLM provider: {str(e)}", exc_info=True)
            # Depending on desired behavior, either raise e or fallback to a default/dummy provider
            raise  # Re-raise the error to halt initialization if provider is crucial

    def generate_sql_from_natural_language(self, question: str) -> Tuple[str, Dict]:
        """
        Takes a natural language question and generates a SQL query
        using the configured LLM provider and schema.
        
        Args:
            question: A natural language question about the database.
            
        Returns:
            A tuple containing the generated SQL query and additional metadata from the LLM provider.
        """
        self.logger.info(f"Delegating SQL generation for question: '{question}' to {config.llm_provider_name} for schema '{self.schema_name}'")
        
        if not self.llm_handler:
            error_message = "LLM handler not initialized. Cannot generate SQL."
            self.logger.error(error_message)
            raise RuntimeError(error_message)
            
        try:
            # The llm_handler is responsible for using the correct schema context
            sql_query, metadata = self.llm_handler.generate_sql(question, self.schema_name)
            self.logger.info(f"SQL query generated by LLM provider: {sql_query}")
            return sql_query, metadata
        except Exception as e:
            # Catching Exception to be generic, specific providers might raise specific errors
            self.logger.error(f"LLM provider failed to generate SQL: {str(e)}", exc_info=True)
            # Re-raise or handle as appropriate for the application
            # For example, return a user-friendly error message in metadata
            # For now, re-raising to ensure errors are visible.
            raise RuntimeError(f"Failed to generate SQL query via {config.llm_provider_name}: {str(e)}")

    def is_natural_language(self, text: str) -> bool:
        """
        Determines if the input is likely natural language rather than SQL,
        delegating to the configured LLM provider.
        
        Args:
            text: The input text.
            
        Returns:
            True if the text is considered natural language, False otherwise.
        """
        if not self.llm_handler:
            self.logger.warning("LLM handler not initialized. Defaulting is_natural_language to False (expecting SQL).")
            # Fallback behavior if handler isn't there - might need adjustment based on desired resilience
            return False 
            
        return self.llm_handler.is_natural_language(text)

# Removed:
# _get_sample_schema() - This logic is now within MistralLLMProvider for the default case
# _build_prompt() - This is now within MistralLLMProvider
# _extract_sql_from_response() - This is now within MistralLLMProvider
# Direct Mistral client initialization and API calls